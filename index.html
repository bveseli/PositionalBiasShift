<!DOCTYPE html>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <title>Positional Biases Shift as Inputs Approach Context Window Limits</title>
    <link rel="stylesheet" href="./files/all.css">
</head>

<body>

<main role="main" id="main" ,="" class="container">
    <br>
    <div class="jumbotron" style="text-align: center">
        <h1 >Positional Biases Shift as Inputs Approach Context Window Limits</h1>
        <a href="https://bveseli.github.io" target="_blank">Blerta Veseli¹</a>,
        <a href="http://simonrazniewski.com" target="_blank">Julian Chibane²</a>,
        <a href="https://research.vu.nl/en/persons/jan-christoph-kalo" target="_blank">Mariya Toneva³</a>,
		<a href="https://people.mpi-inf.mpg.de/~weikum/" target="_blank">Alexander Koller¹</a><br>
        <br>
        <p>
            <a href="https://www.mpi-inf.mpg.de/departments/databases-and-information-systems" target="_blank"> Saarland Informatics Campus, Saarland University, Saarbrücken¹</a>,
            <a href="https://www.bosch-ai.com" target="_blank"> Max Planck Institute for Informatics²</a>,
            <a href="https://www.uva.nl/en" target="_blank"> Max Planck Institute for Software Systems³</a>

        </p>
        <br><br>
        <a href="https://colmweb.org" target="_blank">COLM 2025, Montreal</a>  <br>
    </div>

    <div class="row justify-content-center">
        <div class="column">
            <p class="mb-5"><a class="btn btn-large btn-light" href="test" target="_blank">Paper</a></p>
		</div>
		<div class="column">
            <p class="mb-5"><a class="btn btn-large btn-light" href="https://github.com/bveseli/positional-bias-shift" target="_blank">Dataset</a></p>
		</div>
		<div class="column">
            <p class="mb-5"><a class="btn btn-large btn-light" href="https://github.com/bveseli/positional-bias-shift" target="_blank">Code</a></p>
		</div>
	</div>





  <div class="row">
    <div class="col-12 col-md-12">

        <h2 style="text-align:center">
Abstract
	</h2>

<p class="text-justify">

Large Language Models (LLMs) often struggle to use information across long inputs. Prior work has identified positional biases like the Lost in the Middle (LiM) effect—better performance when key information appears at the beginning (primacy) or end (recency) rather than in the middle—but long-context studies have not consistently replicated these effects. We address this by analyzing input length relative to each model’s context window and find that LiM is strongest when inputs occupy up to 50% of that window. Beyond that point, primacy weakens while recency remains relatively stable, effectively eliminating LiM and revealing a distance-based bias where performance is better when relevant information is closer to the end. Our results also suggest that successful retrieval is a prerequisite for reasoning, and that positional biases in reasoning are largely inherited from retrieval, with implications for long-context tasks, benchmark design, and evaluation.        </p>
    </div>
 </div>
 
 <!---
 <div class="row">
    <div class="col-12 col-md-12">
       <figure style="text-align:center">
       <img  src="./figures/v_shapes-3-1.png" width="60%" class="img-responsive">
       <figcaption style="text-align:left" >The “Lost in the Middle” (LiM) effect describes how models favor information from the beginning (primacy bias) and end (recency bias) over the middle of an input. Our findings reveal that as inputs reach a model's context window size, the primacy bias drops and the LiM effect disappears. We show this effect across models.</b> .
	   </figcaption>
       </figure>
   </div>
 </div>
-->


 
<div class="row">
	<div class="col-12 col-md-12">
		<section id="findings">
		  <h2>Contributions</h2>
		
		  <ol class="findings">
		    <li>
		      <h3>Defining relative input lengths.</h3>
		      <p>Analysis of positional biases using relative input length (proportion of a model’s context window) rather than absolute lengths across models.</p>
		    </li>

		    <li>
		      <h3>Positional biases are consistent across models when analyzed relative to a model's context window size.</h3>
		    </li>
		
		    <li>
		      <h3>LiM is strongest in input lengths up to 50% of a models context window size.</h3>
		      <figure>
		        <img src="./figures/quantitative_measure_of_biases-2-1.png" width="800" alt="What the figure shows">
		      </figure>
		      <p>Brief interpretation of the figure.</p>
		    </li>
		
		    <li>
		      <h3>LiM is strongest in input lengths up to 50% of a models context window size.</h3>
		      <figure>
		        <img src="./figures/v_shapes-3-1.png" width="800" alt="What the figure shows">
		      </figure>
		      <p>Brief interpretation of the figure.</p>
		    </li>
		
		    <li>
		      <h3>With growing input size, primacy bias increasingly collapses, while the recency bias remains stable. This results in a more distance-based bias, i.e. accuracy higher when evidence is nearer the end.</h3>
		      <figure>
		        <img src="./figures/pos_bias_on_cond_probs-2-1.png" width="800" alt="What the figure shows">
		      </figure>
		      <p>Brief interpretation of the figure.</p>
		    </li>
		  </ol>
		</section>
	</div>
</div>
	
<!---
 <div class="row">
    <div class="col-12 col-md-12">
		<ul>
		  <li>Analysis of positional biases using relative input length (proportion of a model’s context window) rather than absolute lengths across models.</li>
		  <li>Positional biases are consistent across models when analyzed relative to a model's context window size.</li>
		  <li>LiM is strongest in input lengths up to 50% of a models context window size.</li>
		  <li>With growing input size, primacy bias increasingly collapses, while the recency bias remains stable. This results in a more distance-based bias, i.e. accuracy higher when evidence is nearer the end.</li>
		  <li>Positional biases in reasoning, specifically the LiM effect, appear to be largely inherited from retrieval.</li>
		</ul>
   </div>
 </div>
-->

    <h2>Citation</h2>
    <pre class="bg-light" style="padding: 5px 10.5px;">	@inproceedings{veseli2025positional,
  title={Positional Biases Shift as Inputs Approach Context Window Limits},
  author={Veseli, Blerta and Chibane, Julian and Toneva, Mariya and Koller, Alexander},
  booktitle={Proceedings of the Conference on Language Modeling (COLM)},
  year={2025}
}</pre>
</main>



</body></html>








